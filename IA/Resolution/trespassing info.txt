 <!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Ayeseri · Clasificar</title>
  <style>
    body{font-family:system-ui,Segoe UI,Roboto,Arial;margin:24px}
    .row{display:flex;gap:12px;align-items:center;margin-bottom:12px}
    #status{margin-top:8px}
    table{border-collapse:collapse;margin-top:12px;width:100%}
    th,td{border:1px solid #ddd;padding:8px}
    th{text-align:left;background:#f5f5f5}
    progress{width:220px;height:10px}
  </style>
</head>
<body>
  <h1>Clasificar errores</h1>
  <div class="row">
    <input id="file" type="file" accept=".csv" />
    <button id="send">Enviar</button>
    <progress id="pg" max="100" value="0" hidden></progress>
  </div>
  <div id="status"></div>
  <div id="preview"></div>

  <script type="module">
    // Si tu API vive en otro dominio, pon su URL aquí:
    const BASE_URL = ""; // "" usa mismo dominio; o por ejemplo "https://tu-api.com"

    const $file   = document.getElementById('file');
    const $send   = document.getElementById('send');
    const $status = document.getElementById('status');
    const $pg     = document.getElementById('pg');
    const $prev   = document.getElementById('preview');

    function msg(text, type='info'){
      const color = type==='error' ? '#b00020' : type==='ok' ? '#0a8f08' : '#333';
      $status.textContent = text;
      $status.style.color = color;
    }

    function renderPreview(data){
      // data esperado desde tu API:
      // { ok: true, sent: N, preview_sent: {texts:[...]}, preview_result: [...] }
      const rows = (data.preview_result || []).map(r => `
        <tr>
          <td>${r.ID_EE ?? ''}</td>
          <td>${escapeHtml(r.message ?? '')}</td>
          <td>${r.label ?? ''}</td>
          <td>${r.confidence ?? ''}</td>
          <td>${r.Load_Date ?? ''} ${r.Load_hour ?? ''}</td>
        </tr>
      `).join('') || `<tr><td colspan="5">Sin preview</td></tr>`;

      $prev.innerHTML = `
        <p><strong>Textos enviados:</strong> ${data.sent ?? '-'}</p>
        <table>
          <thead>
            <tr><th>ID_EE</th><th>Mensaje</th><th>Infotipo</th><th>Conf.</th><th>Fecha/Hora</th></tr>
          </thead>
          <tbody>${rows}</tbody>
        </table>
      `;
    }

    function escapeHtml(s){
      return String(s).replace(/[&<>"']/g, m => ({
        '&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'
      }[m]));
    }

    async function uploadCsv(file){
      // Validaciones básicas
      if (!file) throw new Error('Selecciona un archivo .csv');
      if (!file.name.toLowerCase().endsWith('.csv')) throw new Error('El archivo debe ser .csv');
      if (file.size > 20 * 1024 * 1024) throw new Error('El CSV no puede exceder 20 MB');

      const fd = new FormData();
      fd.append('file', file);

      // Mostrar “progreso” aproximado (no hay progreso real con fetch sin streams)
      $pg.hidden = false; $pg.value = 25; msg('Subiendo archivo…');

      const res = await fetch((BASE_URL || '') + '/ClasifyMethod', {
        method: 'POST',
        body: fd,
      });

      $pg.value = 75; msg('Procesando…');

      const data = await res.json().catch(() => ({}));
      if (!res.ok) {
        throw new Error(data?.error || `HTTP ${res.status}`);
      }

      $pg.value = 100; setTimeout(()=>{$pg.hidden = true; $pg.value = 0;}, 300);
      return data;
    }

    $send.addEventListener('click', async () => {
      try {
        $prev.innerHTML = '';
        const file = $file.files[0];
        const data = await uploadCsv(file);
        msg('Listo 🎉', 'ok');
        renderPreview(data);
        console.log('Respuesta API:', data);
      } catch (e) {
        console.error(e);
        msg(e.message || 'Error al enviar', 'error');
        $pg.hidden = true; $pg.value = 0;
      }
    });
  </script>
</body>
</html>






# -*- coding: utf-8 -*-
import os
import re
import unicodedata
import joblib
import nltk
from typing import List, Dict, Any, Tuple

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import ComplementNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, confusion_matrix
)

# ----- TU CONECTOR -----
from API_connection import GetErros_FromAPI  # ojo con el nombre "Erros"

# ----- RUTAS -----
PIPELINE_PATH = "modelo_infotipo_pipeline.pkl"

# ---------------------------------------------------------------------
# STOPWORDS + STEMMER
# ---------------------------------------------------------------------
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def _ensure_stopwords() -> set:
    try:
        stopwords.words("english")
    except LookupError:
        nltk.download("stopwords")

    # Mezcla inglés + español (útil si tus errores vienen bilingües)
    sw_en = set(stopwords.words("english"))
    try:
        sw_es = set(stopwords.words("spanish"))
    except LookupError:
        nltk.download("stopwords")
        sw_es = set(stopwords.words("spanish"))
    return sw_en.union(sw_es)

STOP_WORDS = _ensure_stopwords()
STEMMER = PorterStemmer()  # si quieres Snowball para ES, lo cambiamos luego

# ---------------------------------------------------------------------
# PREPROCESADO (normaliza acentos, minúsculas, quita signos)
# ---------------------------------------------------------------------
_punct_regex = re.compile(r"[^\w\s]")  # conserva alfanum y espacios

def _normalize(text: str) -> str:
    # Maneja None, ints, etc.
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    text = text.lower()
    # Normaliza acentos (á -> a)
    text = unicodedata.normalize("NFKD", text)
    text = "".join(ch for ch in text if not unicodedata.combining(ch))
    # Elimina signos
    text = _punct_regex.sub(" ", text)
    # Colapsa espacios
    text = re.sub(r"\s+", " ", text).strip()
    return text

def _analyzer(doc: str) -> List[str]:
    """Analyzer para TfidfVectorizer: normaliza, tokeniza, quita stopwords y aplica stemming."""
    doc = _normalize(doc)
    tokens = doc.split()
    toks = []
    for t in tokens:
        if not t or t in STOP_WORDS:
            continue
        # stemming inglés (si quieres ES: SnowballStemmer('spanish'))
        toks.append(STEMMER.stem(t))
    return toks

# ---------------------------------------------------------------------
# UTILIDADES PARA LEER DE LA API
# ---------------------------------------------------------------------
def _extract_error_message(item: Dict[str, Any]) -> str:
    # Soporta varios campos: 'Error Message', 'Error_Message', 'Error', 'Error_message'
    return (item.get("Error Message")
            or item.get("Error_Message")
            or item.get("Error_message")
            or item.get("Error")
            or "")

def _extract_label(item: Dict[str, Any]):
    # Soporta 'ID_Infotype', 'Infotype_IND', 'Infotype', 'IT_affected'
    return (item.get("ID_Infotype")
            or item.get("Infotype_IND")
            or item.get("Infotype")
            or item.get("IT_affected"))

def _load_training_data() -> Tuple[List[str], List[Any]]:
    data = GetErros_FromAPI()  # se espera lista[dict]
    if not data:
        raise ValueError("GetErros_FromAPI() devolvió vacío. No hay datos para entrenar.")

    X_text, y = [], []
    for it in data:
        msg = _extract_error_message(it)
        lbl = _extract_label(it)
        if not msg or lbl is None:
            continue
        X_text.append(msg)
        y.append(lbl)

    if len(X_text) < 2:
        raise ValueError("Datos insuficientes para entrenar (se requieren al menos 2 ejemplos).")
    if len(set(y)) < 2:
        raise ValueError("Solo hay una clase en las etiquetas; se requieren al menos 2 clases.")

    return X_text, y

# ---------------------------------------------------------------------
# CONSTRUCCIÓN DEL PIPELINE
# ---------------------------------------------------------------------
def _build_pipeline() -> Pipeline:
    """
    Devuelve un Pipeline: TfidfVectorizer(ngram 1-2) + ComplementNB.
    - Usamos analyzer personalizado para stemming y stopwords bilingües.
    """
    vect = TfidfVectorizer(
        analyzer=_analyzer,
        ngram_range=(1, 2),
        min_df=2,        # ignora términos muy raros
        max_df=0.90,     # ignora términos demasiado frecuentes
        # lowercase=False  # no necesario, ya normalizamos
    )
    clf = ComplementNB()
    pipe = Pipeline([
        ("vect", vect),
        ("clf", clf),
    ])
    return pipe

# ---------------------------------------------------------------------
# ENTRENAMIENTO, EVALUACIÓN Y PERSISTENCIA
# ---------------------------------------------------------------------
def reentrenar_modelo(test_size: float = 0.2, random_state: int = 42) -> dict:
    """
    Reentrena el modelo desde la API, evalúa y guarda el pipeline en PIPELINE_PATH.
    Devuelve un dict con métricas para evidencia (Módulo 2).
    """
    X, y = _load_training_data()
    # Stratified Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    pipe = _build_pipeline()
    pipe.fit(X_train, y_train)

    # Evaluación
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)
    f1_weighted = f1_score(y_test, y_pred, average="weighted", zero_division=0)
    cls_report = classification_report(y_test, y_pred, zero_division=0)
    cm = confusion_matrix(y_test, y_pred)

    # Guardar
    joblib.dump(pipe, PIPELINE_PATH)

    return {
        "accuracy": acc,
        "f1_macro": f1_macro,
        "f1_weighted": f1_weighted,
        "classification_report": cls_report,
        "confusion_matrix": cm.tolist(),
        "test_size": test_size,
        "n_samples_train": len(X_train),
        "n_samples_test": len(X_test),
        "n_classes": len(set(y))
    }

def cargar_modelo() -> Pipeline:
    """
    Carga el pipeline entrenado desde disco. Si no existe, entrena y lo guarda.
    """
    if not os.path.exists(PIPELINE_PATH):
        # Entrena y guarda si no existe
        reentrenar_modelo()
    return joblib.load(PIPELINE_PATH)

# ---------------------------------------------------------------------
# PREDICCIÓN
# ---------------------------------------------------------------------
def predecir_infotipo(mensaje_error: str):
    """
    Devuelve la etiqueta de infotipo predicha para un mensaje de error.
    """
    if not mensaje_error:
        raise ValueError("mensaje_error está vacío o None.")
    pipe = cargar_modelo()
    return pipe.predict([mensaje_error])[0]

# ---------------------------------------------------------------------
# USO OPCIONAL DESDE CLI
# ---------------------------------------------------------------------
if __name__ == "__main__":
    # 1) Reentrenar y mostrar métricas
    try:
        metrics = reentrenar_modelo()
        print("\n=== Métricas de evaluación ===")
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"F1-macro: {metrics['f1_macro']:.4f}")
        print(f"F1-weighted: {metrics['f1_weighted']:.4f}")
        print("\nClassification Report:\n", metrics["classification_report"])
        print("Confusion Matrix:", metrics["confusion_matrix"])
    except Exception as e:
        print("⚠️ No se pudo reentrenar:", e)

    # 2) Predicción de ejemplo
    try:
        ejemplo = "Formatting error in the field P0001-ANSVH"
        print("\nPredicción de ejemplo ->", predecir_infotipo(ejemplo))
    except Exception as e:
        print("⚠️ No se pudo predecir:", e)